{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=53\n",
    "channels=3\n",
    "train_ratio=0.8\n",
    "val_ratio=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label for category reception is 0\n",
      "306\n",
      "38\n",
      "39\n",
      "label for category conference is 1\n",
      "174\n",
      "21\n",
      "23\n",
      "total train = 482\n",
      "total val = 57\n",
      "total test = 62\n"
     ]
    }
   ],
   "source": [
    "label = 0\n",
    "train = []\n",
    "val = []\n",
    "test = []\n",
    "folderpath=\"/home/justdial/cnn/items\"\n",
    "for cat in os.listdir(folderpath):\n",
    "    print(\"label for category {} is {}\".format(cat,label))\n",
    "    images = os.listdir(os.path.join(folderpath,cat))\n",
    "    totalimages = len(images)\n",
    "    num_train = int(totalimages*train_ratio)\n",
    "    num_val = int(totalimages*val_ratio)\n",
    "    num_test = totalimages - (num_train + num_val)\n",
    "    \n",
    "    print(num_train)\n",
    "    print(num_val)\n",
    "    print(num_test)\n",
    "    \n",
    "    for i,img in enumerate(images):\n",
    "        if(i<=num_train):\n",
    "            train.append([os.path.join(os.path.join(folderpath,cat),img),label])\n",
    "        elif(i>num_train and i<num_train+num_val):\n",
    "             val.append([os.path.join(os.path.join(folderpath,cat),img),label])\n",
    "        elif(i>=num_train+num_val and i<=totalimages):\n",
    "             test.append([os.path.join(os.path.join(folderpath,cat),img),label])\n",
    "    label+=1\n",
    "\n",
    "print(\"total train =\", len(train))\n",
    "print(\"total val =\", len(val))\n",
    "print(\"total test =\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords_folder_path=\"/home/justdial/cnn/tfrecords\"\n",
    "writer1=tf.python_io.TFRecordWriter(tfrecords_folder_path + \"/tfrecordtrain\" )\n",
    "writer2=tf.python_io.TFRecordWriter(tfrecords_folder_path + \"/tfrecordval\" )\n",
    "writer3=tf.python_io.TFRecordWriter(tfrecords_folder_path + \"/tfrecordtest\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_bytes(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def wrap_int64(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary(image):\n",
    "    img = Image.open(image)\n",
    "    img = img.resize((image_size,image_size),Image.ANTIALIAS)\n",
    "    img = img.tobytes()\n",
    "    \n",
    "    if(len(img) == image_size*image_size*channels):\n",
    "        return img\n",
    "    else:\n",
    "        print(\"image not converted {}\".format(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecord(img_bytes,label):\n",
    "    data = {\n",
    "        'image' : wrap_bytes(img_bytes),\n",
    "        'label' : wrap_int64(label)}\n",
    "    feature = tf.train.Features(feature=data)\n",
    "    example = tf.train.Example(features=feature)\n",
    "    serialized = example.SerializeToString()\n",
    "    return serialized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image not converted /home/justdial/cnn/items/reception/21053888.png\n",
      "image not converted /home/justdial/cnn/items/conference/51203355.png\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    shuffle(train)\n",
    "    shuffle(val)\n",
    "    shuffle(test)\n",
    "\n",
    "try: \n",
    "    for i in train:\n",
    "        image = i[0]\n",
    "        label = i[1]\n",
    "        img_bytes = get_binary(image)\n",
    "        serialized = create_tfrecord(img_bytes,label)\n",
    "        writer1.write(serialized)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    for i in val:\n",
    "        image = i[0]\n",
    "        label = i[1]\n",
    "        img_bytes = get_binary(image)\n",
    "        serialized = create_tfrecord(img_bytes,label)\n",
    "        writer2.write(serialized)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    for i in test:\n",
    "        image = i[0]\n",
    "        label = i[1]\n",
    "        img_bytes = get_binary(image)\n",
    "        serialized = create_tfrecord(img_bytes,label)\n",
    "        writer3.write(serialized)\n",
    "except:\n",
    "    pass\n",
    "        \n",
    "writer1.close()\n",
    "writer2.close()\n",
    "writer3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(serialized):\n",
    "    features={\n",
    "        'image':tf.FixedLenFeature([],tf.string),\n",
    "        'label':tf.FixedLenFeature([],tf.int64)\n",
    "    }\n",
    "    parsed_example=tf.parse_single_example(serialized=serialized,\n",
    "                                          features=features)\n",
    "    raw_image=parsed_example['image']\n",
    "    image=tf.decode_raw(raw_image,tf.uint8)\n",
    "    \n",
    "    image = tf.reshape(image,shape=[image_size,image_size,channels])\n",
    "\n",
    "    image=tf.cast(image,tf.float64)    \n",
    "    label=parsed_example['label']\n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def load_tfrecord:\n",
    "    \n",
    "# traindataset=tf.data.TFRecordDataset(filenames='/home/justdial/cnn/tfrecords/tfrecordtrain')\n",
    "# traindataset=traindataset.map(parse)\n",
    "# # print((traindataset))\n",
    "\n",
    "# valdataset=tf.data.TFRecordDataset(filenames='/home/justdial/cnn/tfrecords/tfrecordval')\n",
    "# valdataset=valdataset.map(parse)\n",
    "# #print(len(valdataset))\n",
    "\n",
    "# testdataset=tf.data.TFRecordDataset(filenames='/home/justdial/cnn/tfrecords/tfrecordtest')\n",
    "# testdataset=testdataset.map(parse)\n",
    "# #print(len(testdataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(filenames,train, batch_size=32,buffer_size=2048):\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(filenames=filenames)\n",
    "    dataset = dataset.map(parse)\n",
    "    if train:\n",
    "        dataset=dataset.shuffle(buffer_size=buffer_size)\n",
    "        num_repeat=None\n",
    "    else:\n",
    "        num_repeat=1\n",
    "    dataset = dataset.repeat(num_repeat)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    img_batch,Y = iterator.get_next()\n",
    "    X={'image':img_batch}\n",
    "\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator= tf.data.Iterator.from_structure(output_types = traindataset.output_types, \n",
    "#                                           output_shapes = traindataset.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_init = iterator.make_initializer(traindataset)\n",
    "# val_init = iterator.make_initializer(valdataset)\n",
    "# test_init = iterator.make_initializer(testdataset)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(train_init)\n",
    "# #     sess.run(val_init)\n",
    "# #     sess.run(test_init)\n",
    "\n",
    "# img_batch,Y = iterator.get_next()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_inp_fun():\n",
    "    return input_fn(filenames='/home/justdial/cnn/tfrecords/tfrecordtrain',train=True)\n",
    "\n",
    "def val_inp_fun():\n",
    "    return input_fn(filenames='/home/justdial/cnn/tfrecords/tfrecordval',train=False)\n",
    "\n",
    "def test_inp_fun():\n",
    "    return input_fn(filenames='/home/justdial/cnn/tfrecords/tfrecordtrain',train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features,labels,mode,params):\n",
    "    X = features['image']\n",
    "    net = tf.reshape(X,[-1,image_size,image_size,channels])\n",
    "    \n",
    "    #first convolutional layer\n",
    "    net = tf.layers.conv2d(inputs = net ,\n",
    "                           name = 'conv1',\n",
    "                           filters = 32,\n",
    "                           kernel_size = 3,\n",
    "                           padding = 'same',\n",
    "                           activation =  tf.nn.relu)\n",
    "    \n",
    "    net = tf.layers.max_pooling2d(inputs = net,\n",
    "                                 pool_size=2,\n",
    "                                 strides=1)\n",
    "    \n",
    "    #second convolutional layer\n",
    "    net = tf.layers.conv2d(inputs = net ,\n",
    "                           name = 'conv2',\n",
    "                           filters = 32,\n",
    "                           kernel_size = 3,\n",
    "                           padding = 'same',\n",
    "                           activation =  tf.nn.relu)\n",
    "    \n",
    "    net = tf.layers.max_pooling2d(inputs = net,\n",
    "                                 pool_size=2,\n",
    "                                 strides =1)\n",
    "    \n",
    "    #flatten\n",
    "    net = tf.contrib.layers.flatten(net)\n",
    "    \n",
    "    #first fully connected layer\n",
    "    net = tf.layers.dense(inputs = net,\n",
    "                         name = 'fc1',\n",
    "                         units = 128,\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    #second fully connected layer\n",
    "    net = tf.layers.dense(inputs = net,\n",
    "                         name = 'fc2',\n",
    "                         units = 2, #num_classes\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    y_pred = tf.nn.softmax(logits = net)\n",
    "    y_pre_cls = tf.argmax(y_pred,axis=1)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        spec = tf.estimator.EstimatorSpec(mode = mode ,\n",
    "                                         predictions = y_pred_cls)\n",
    "        return spec\n",
    "    else:\n",
    "        eval_metric_ops = {\"accuracy\": tf.metrics.accuracy(\n",
    "                    labels=labels, predictions=predictions[\"classes\"])}\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y_pred,\n",
    "                                                                     labels=labels)\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate = params['learning_rate'])\n",
    "            train_op = optimizer.minimize(loss=loss,\n",
    "                                         global_step=tf.train.get_global_step())\n",
    "            return tf.estimator.EstimatorSpec(mode=mode ,loss=loss, train_op =train_op ,accuracy=eval_metric_ops)\n",
    "        else:\n",
    "        \n",
    "#             eval_metric_ops = {\"accuracy\": tf.metrics.accuracy(\n",
    "#                     labels=labels, predictions=predictions[\"classes\"])}\n",
    "            #print(eval_metric_ops)\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'learning_rate':1e-3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                              params = params,\n",
    "                              model_dir = \"./Checkpoints_example/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0711 18:02:20.486825 140469486601984 deprecation.py:323] From /home/justdial/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0711 18:02:20.570307 140469486601984 deprecation.py:323] From <ipython-input-10-6face38b7c61>:12: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "W0711 18:02:20.583705 140469486601984 deprecation.py:323] From <ipython-input-12-4556f4f71102>:11: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0711 18:02:20.586757 140469486601984 deprecation.py:506] From /home/justdial/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0711 18:02:20.724065 140469486601984 deprecation.py:323] From <ipython-input-12-4556f4f71102>:15: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W0711 18:02:21.092151 140469486601984 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0711 18:02:21.092739 140469486601984 deprecation.py:323] From /home/justdial/miniconda3/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W0711 18:02:21.283661 140469486601984 deprecation.py:323] From <ipython-input-12-4556f4f71102>:36: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-656b1f046afa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_inp_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1156\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1186\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m-> 1188\u001b[0;31m           features, labels, ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m   1189\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-4556f4f71102>\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         eval_metric_ops = {\"accuracy\": tf.metrics.accuracy(\n\u001b[0;32m---> 53\u001b[0;31m                     labels=labels, predictions=predictions[\"classes\"])}\n\u001b[0m\u001b[1;32m     54\u001b[0m         cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y_pred,\n\u001b[1;32m     55\u001b[0m                                                                      labels=labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "model.train(input_fn = train_inp_fun, steps =200)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(input_fn =val_inp_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTest set accuracy: {}\\n'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
